{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the packages we need for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "print 'loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load a restricted version of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_20newsgroups in module sklearn.datasets.twenty_newsgroups:\n",
      "\n",
      "fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
      "    Load the filenames and data from the 20 newsgroups dataset.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset: 'train' or 'test', 'all', optional\n",
      "        Select the dataset to load: 'train' for the training set, 'test'\n",
      "        for the test set, 'all' for both, with shuffled ordering.\n",
      "    \n",
      "    data_home: optional, default: None\n",
      "        Specify an download and cache folder for the datasets. If None,\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    categories: None or collection of string or unicode\n",
      "        If None (default), load all the categories.\n",
      "        If not None, list of category names to load (other categories\n",
      "        ignored).\n",
      "    \n",
      "    shuffle: bool, optional\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    random_state: numpy random number generator or seed integer\n",
      "        Used to shuffle the dataset.\n",
      "    \n",
      "    download_if_missing: optional, True by default\n",
      "        If False, raise an IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    remove: tuple\n",
      "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "        these are kinds of text that will be detected and removed from the\n",
      "        newsgroup posts, preventing classifiers from overfitting on\n",
      "        metadata.\n",
      "    \n",
      "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "        ends of posts that look like signatures, and 'quotes' removes lines\n",
      "        that appear to be quoting another post.\n",
      "    \n",
      "        'headers' follows an exact standard; the other filters are not always\n",
      "        correct.\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'target', 'target_names', 'filenames']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print help(fetch_20newsgroups)\n",
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "['DESCR', 'data', 'target', 'target_names', 'filenames']\n",
      "2257\n",
      "2257\n",
      "[1 1 3 ..., 2 2 2]\n",
      "1 comp.graphics\n",
      "1 comp.graphics\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "2 sci.med\n",
      "2 sci.med\n",
      "2 sci.med\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "print twenty_train.target_names # these are the names of the newsgroups\n",
    "print twenty_train.keys() # these are the items available in the 'bunch'\n",
    "print len(twenty_train.data)\n",
    "print len(twenty_train.filenames)\n",
    "\n",
    "print twenty_train.target\n",
    "\n",
    "for t in twenty_train.target[:10]:\n",
    "    print t, twenty_train.target_names[t]\n",
    "\n",
    "# first three lines of the first record (data point, which is a newsgroup document)\n",
    "print \"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tokenize and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35787)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "4690\n",
      "set([])\n",
      "['only']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words=[\"only\"])\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print X_train_counts.shape\n",
    "\n",
    "print type(X_train_counts)\n",
    "\n",
    "### write something here\n",
    "\n",
    "### end of space\n",
    "\n",
    "print count_vect.vocabulary_.get(u'algorithm')\n",
    "print count_vect.stop_words_ # no stop words : - /\n",
    "print count_vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35787)\n",
      "[0, 5195]   (0, 0)\t0.0753778361444\n",
      "(2257, 35787)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfTransformer' object has no attribute '_idf_diag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a0294647a18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idf_diag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"[0, 5195]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5195\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfTransformer' object has no attribute '_idf_diag'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts) \n",
    "X_train_tf = tf_transformer.transform(X_train_counts) \n",
    "print X_train_tf.shape\n",
    "\n",
    "print \"[0, 5195]\", X_train_tf.getrow(0).getcol(5195)\n",
    "\n",
    "# the above does not do much, since idf is turned off\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print X_train_tfidf.shape\n",
    "\n",
    "print tf_transformer._idf_diag\n",
    "\n",
    "print \"[0, 5195]\", X_train_tf.getrow(0).getcol(5195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a1d457c61277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "print \"hello world\"\n",
    "\n",
    "a = 1\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command printed hello to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
