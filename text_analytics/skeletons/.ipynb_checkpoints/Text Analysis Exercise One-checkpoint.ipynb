{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the packages we need for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "print 'loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load a restricted version of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_20newsgroups in module sklearn.datasets.twenty_newsgroups:\n",
      "\n",
      "fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
      "    Load the filenames and data from the 20 newsgroups dataset.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset: 'train' or 'test', 'all', optional\n",
      "        Select the dataset to load: 'train' for the training set, 'test'\n",
      "        for the test set, 'all' for both, with shuffled ordering.\n",
      "    \n",
      "    data_home: optional, default: None\n",
      "        Specify an download and cache folder for the datasets. If None,\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    categories: None or collection of string or unicode\n",
      "        If None (default), load all the categories.\n",
      "        If not None, list of category names to load (other categories\n",
      "        ignored).\n",
      "    \n",
      "    shuffle: bool, optional\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    random_state: numpy random number generator or seed integer\n",
      "        Used to shuffle the dataset.\n",
      "    \n",
      "    download_if_missing: optional, True by default\n",
      "        If False, raise an IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    remove: tuple\n",
      "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "        these are kinds of text that will be detected and removed from the\n",
      "        newsgroup posts, preventing classifiers from overfitting on\n",
      "        metadata.\n",
      "    \n",
      "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "        ends of posts that look like signatures, and 'quotes' removes lines\n",
      "        that appear to be quoting another post.\n",
      "    \n",
      "        'headers' follows an exact standard; the other filters are not always\n",
      "        correct.\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'target', 'target_names', 'filenames']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print help(fetch_20newsgroups)\n",
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "['DESCR', 'data', 'target', 'target_names', 'filenames']\n",
      "2257\n",
      "2257\n",
      "[1 1 3 ..., 2 2 2]\n",
      "1 comp.graphics\n",
      "1 comp.graphics\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "3 soc.religion.christian\n",
      "2 sci.med\n",
      "2 sci.med\n",
      "2 sci.med\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "print twenty_train.target_names # these are the names of the newsgroups\n",
    "print twenty_train.keys() # these are the items available in the 'bunch'\n",
    "print len(twenty_train.data)\n",
    "print len(twenty_train.filenames)\n",
    "\n",
    "print twenty_train.target\n",
    "\n",
    "for t in twenty_train.target[:10]:\n",
    "    print t, twenty_train.target_names[t]\n",
    "\n",
    "# first three lines of the first record (data point, which is a newsgroup document)\n",
    "print \"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tokenize and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35787)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "4690\n",
      "set([])\n",
      "['only']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words=[\"only\"])\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print X_train_counts.shape\n",
    "\n",
    "print type(X_train_counts)\n",
    "\n",
    "### write something here\n",
    "\n",
    "### end of space\n",
    "\n",
    "print count_vect.vocabulary_.get(u'algorithm')\n",
    "print count_vect.stop_words_ # no stop words : - /\n",
    "print count_vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35787)\n",
      "[0, 5195]   (0, 0)\t0.0753778361444\n",
      "(2257, 35787)\n",
      "[0, 5195]   (0, 0)\t0.0753778361444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts) \n",
    "X_train_tf = tf_transformer.transform(X_train_counts) \n",
    "print X_train_tf.shape\n",
    "\n",
    "print \"[0, 5195]\", X_train_tf.getrow(0).getcol(5195)\n",
    "\n",
    "# the above does not do much, since idf is turned off\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print X_train_tfidf.shape\n",
    "\n",
    "print \"[0, 5195]\", X_train_tf.getrow(0).getcol(5195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20537)\t0.809401056239\n",
      "  (0, 18474)\t0.255894264685\n",
      "  (0, 15521)\t0.528571712694\n",
      "  (1, 32141)\t0.0891122234242\n",
      "  (1, 23789)\t0.600483554876\n",
      "  (1, 23733)\t0.128412754679\n",
      "  (1, 18474)\t0.100340184397\n",
      "  (1, 15628)\t0.677839555469\n",
      "  (1, 14048)\t0.381384400353\n",
      "  (2, 15521)\t0.45724798045\n",
      "  (2, 14048)\t0.841388285598\n",
      "  (2, 5410)\t0.288079914666\n",
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "'god gpus are fast!' => soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target) \n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast', 'god gpus are fast!']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "print X_new_tfidf\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command printed hello to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...False,\n",
      "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...False,\n",
      "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n"
     ]
    }
   ],
   "source": [
    "# a pipeline \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "print text_clf\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we made a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 ..., 2 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print predicted\n",
    "np.mean(predicted == twenty_test.target)\n",
    "\n",
    "# we should practice some plotting here to look into these results more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
